require(data.table); require(lubridate); require(caret); require(sqldf); require(xgboost); require(sqldf); require(xlsx); require(Matrix)

train_raw <- fread(input = "D:\\kaggle\\HOMESITE\\Data\\train.csv", data.table = F)

response <- train_raw$QuoteConversion_Flag

response <- as.matrix(response)

train_raw$QuoteConversion_Flag <- NULL

train_raw$QuoteNumber <- NULL


test_raw <- fread(input = "D:\\kaggle\\HOMESITE\\Data\\test.csv", data.table = F)

id <- test_raw$QuoteNumber

test_raw$QuoteNumber <- NULL


tmp <- rbind(train_raw, test_raw)

tmp$Original_Quote_Date <- as.Date(tmp$Original_Quote_Date)

tmp$month <- as.integer(format(tmp$Original_Quote_Date, "%m"))

tmp$year <- as.integer(format(tmp$Original_Quote_Date, "%y"))

tmp$day <- weekdays(as.Date(tmp$Original_Quote_Date))

continous_field <- tmp$SalesField8

tmp$SalesField8 <- NULL

tmp$Original_Quote_Date <- NULL

tmp <- as.matrix(tmp)

# group data => create combinations of a given order
groupData <- function(xmat, degree)
{
  require(foreach, quietly = T)
  # indices of combinations
  xind <- combn(1:ncol(xmat), degree)
  
  # storage structure for the result
  agx <- foreach(ii = 1:ncol(xind), .combine = cbind ) %do%
{
  x <- xmat[,xind[1,ii]]
  for (jj in 2:nrow(xind))
  {
    x <- paste(x, xmat[,xind[jj,ii]], sep = "_")
  }
  x
}
  colnames(agx) <- paste(paste("f", degree, sep = ""), 1:ncol(agx), sep = "_")
  return(agx)
}


# running into memory issues

# double <- groupData(tmp, 2)

tmp_mat <- cbind(tmp)#, double)

tmp_mat_train <- tmp_mat[1:260753 , ]

tmp_mat_test = tmp_mat[(260753+1):434589, ]

response_test = numeric()

# need tmp_mat_train, tmp_mat_test, response, response_test : remove everything

#function for forward stepwise logistic regression fitting to determine optimal features to encode


set.seed(508)

require(glmnet, quietly = T)

require(Matrix, quietly = T)

#initialize optimization metrics-----------------------------------------------------------------------------------------------

cv_max_auc = 0.5 #target minimum.... represents random guess

cv_fold_auc = numeric()

cv_train_auc = numeric()

#initialize feature holding-----------------------------------------------------------------------------------------------------

best_col = numeric()

num_features = numeric()

# big inefficient for loop that does everything -------------------------------------------------------------------------------

for(i in 1:ncol(tmp_mat_train)) 

{

# add columns selected + iteration column

  colName = colnames(tmp_mat_train)[c(best_col, i)] 
  
  vars = as.data.frame(tmp_mat_train[,c(best_col, i)])
  
  colnames(vars) = colName
  
  #encode into sparse model               
  
  vars = sparse.model.matrix(~ . - 1, data = vars)                     
  
  #10 fold logistic regression w/ lasso reg ran to obtain max mean AUC on validation set
  
  for(j in 1:10) 
  
  {
  
    cv_train = cv.glmnet(x = vars, y = response[,1], family = "binomial", type.measure = "auc")
  
    cv_fold_auc[j] = max(cv_train$cvm)
  
  }
  
  cv_train_auc[i] = mean(cv_fold_auc)
  
  #reset cv fold auc
  
  cv_fold_auc = numeric()
  
  #determining if new column is useful.  if so, adding to the model and raising auc bar
  
  if(cv_train_auc[i] > cv_max_auc)
  
  {
    
    #for next iteration: know indecies of the columns to keep
  
    best_col = c(best_col, i)
    
    #store how many important features from the current set to plot against auc
  
    best_features = which(coef(cv_train, s = cv_train$lambda.min) > 0)
  
    num_features[i] = length(best_features)
    
    #raise auc bar
  
    cv_max_auc = cv_train_auc[i]
    
  }
  
  #live update
  
  for(k in 1)
  
  {
  
  print(cat('Feature Loop', i, 'complete.  Max validation AUC:', cv_max_auc, 'Number of features:', num_features[i]))
  
  }
  
}
  
print(best_col) 

tmp_mat_train = tmp_mat_train[,best_col]

# add model training step


##############################################################################################################################

# new approach---------------------------------------------------------------------------------------------------------------

# two way and three way counts

# sparse matrix

# run lasso, same step as listed above

# run xgboost on the selected features

#############################################################################################################################

# new appraoch ------------------------------------------------------------------------------------------------------------

# factor to integer

### Creates a set of new columns for your data set that summarize factor levels by descriptive statistics such as mean,

### sd, skewness etc.

### Variables : train = training set

###             test = test set

###             response = response variable (Hazard in this competition)

###             variables = vector of column names you wise to summarize (T1_V4 for example). Must be strings.

###             metrics = vector of desctriptive statistics you wish to compute for each factor level. Must Be Strings.

### ex: factorToNumeric(train, test, "Hazard", "T1_V4", "mean") will return a column of hazard means by factor level

###     in column T1_V4

### You can specify the test parameter as train to get a column to add to your training set

###### REQUIRES package qdapTools  #####

library(qdapTools)



factorToNumeric <- function(train, test, response, variables, metrics){

  temp <- data.frame(c(rep(0,nrow(test))), row.names = NULL)

  for (variable in variables){

    for (metric in metrics) {

      x <- tapply(train[, response], train[,variable], metric)

      x <- data.frame(row.names(x),x, row.names = NULL)

      temp <- data.frame(temp,round(lookup(test[,variable], x),2))

      colnames(temp)[ncol(temp)] <- paste(metric,variable, sep = "_")

    }

  }

  return (temp[,-1])

}



#### Sample Usage #### 

train <- read.csv("../input/train.csv")

test <- read.csv("../input/test.csv")

#### Returns mean, median, and sd for factor T1_V4 by factor level to be added to the training set.

data.frame(head(train$T1_V4))

head(factorToNumeric(train, train, "Hazard", "T1_V4", c("mean","median","sd")))

#### Returns mean, median, and sd for factor T1_V4 by factor level to be added to the test set.

data.frame(head(test$T1_V4))

head(factorToNumeric(train, test, "Hazard", "T1_V4", c("mean","median","sd")))

##############################################################################################################################

# new approach--------------------------------------------------------------------------------------------------------------

# add interaction features ( all types + | / | - | * ) to the best performing model

# use add new features function ( something similar to obtain the end result )

addAggFeatures <- function(data) {

# add new features
  
  mutate(data, feat_sum = as.integer(rowSums(data[, 1:ncol(tmp_new)])),  # count sum of features by row
         
         feat_var = as.integer(apply(data[, 1:ncol(tmp_new)], 1, var)),  # variance of features by row
         
         feat_filled = as.integer(rowSums(data[, 1:ncol(tmp_new)] != 0))  # count no. of non-empty features
  )
  
}

##############################################################################################################################


#12/03/2015

# loop around fr different seed settings

set.seed(1718)

# set.seed(12032015)

require(data.table); require(lubridate); require(caret); require(sqldf); require(xgboost); require(sqldf); require(xlsx); require(Matrix)

train_raw <- fread(input = "D:\\kaggle\\HOMESITE\\Data\\train.csv", data.table = F)

response <- train_raw$QuoteConversion_Flag

train_raw$QuoteConversion_Flag <- NULL

train_raw$QuoteNumber <- NULL


test_raw <- fread(input = "D:\\kaggle\\HOMESITE\\Data\\test.csv", data.table = F)

id <- test_raw$QuoteNumber

test_raw$QuoteNumber <- NULL



tmp <- rbind(train_raw, test_raw)

tmp$Original_Quote_Date <- as.Date(tmp$Original_Quote_Date)

tmp$month <- as.integer(format(tmp$Original_Quote_Date, "%m"))

tmp$year <- as.integer(format(tmp$Original_Quote_Date, "%y"))

tmp$day <- weekdays(as.Date(tmp$Original_Quote_Date))

tmp$week <- week((as.Date(tmp$Original_Quote_Date)))

tmp$date <- (((tmp$year * 52 ) + tmp$week) %% 4)

# check before editing

#tmp$Original_Quote_Date <- NULL

tmp[is.na(tmp)] <- -1

feature.names <- names(tmp)


for (f in feature.names) {
  
  if (class(tmp[[f]]) == "character") {
    
    levels <- unique(c(tmp[[f]]))
    
    tmp[[f]] <- as.integer(factor(tmp[[f]], levels=levels))
    }
}


train <- tmp[1:260753 , ]

test = tmp[(260753+1):434589, ]


nrow(train)

h<-sample(nrow(train),2000)

dval<-xgb.DMatrix(data=data.matrix(train[h,]),label=response[h])

#dtrain<-xgb.DMatrix(data=data.matrix(tra[-h,]),label=train$QuoteConversion_Flag[-h])

dtrain <-xgb.DMatrix(data=data.matrix(train),label=response)

watchlist<-list(val=dval,train=dtrain)

cl <- makeCluster(2); registerDoParallel(cl)

param <- list(  objective           = "binary:logistic", 
                booster = "gbtree",
                eval_metric = "auc",
                eta                 = 0.023, # 0.06, #0.01,
                max_depth           = 6, #changed from default of 8
                subsample           = 0.83, # 0.7
                colsample_bytree    = 0.77 # 0.7
                #num_parallel_tree   = 2
                # alpha = 0.0001, 
                # lambda = 1
)

clf <- xgb.train(   params              = param, 
                    data                = dtrain, 
                    nrounds             = 1800, 
                    verbose             = 0,  #1
                    #early.stop.round    = 150,
                    #watchlist           = watchlist,
                    maximize            = FALSE
)



pred <- predict(clf, data.matrix(test[,feature.names]))

submission <- data.frame(QuoteNumber=test$QuoteNumber, QuoteConversion_Flag=pred1)

write_csv(submission, "D:\\kaggle\\HOMESITE\\submission\\12032015_1.csv")

# CV <- xgb.cv()

xgb.cv(params = param, 
       
       data = dtrain, 
       
       nrounds = 50, 
       
       nfold = 10, 
       
       showsd = T, 
       
       verbose = 1, 
       
       maximize = F
       )
       
       
# edit --- added new features



# new vars

##-------------------------------------------------------------------------------------------------------------------------------------

# count sum of features by row
mutate(tmp, feat_sum <- as.integer(rowSums(tmp[, 1:ncol(tmp)])),  

# variance of features by row
feat_var <- as.integer(apply(tmp[, 1:ncol(tmp)], 1, var)),  

# count no. of non-empty features
feat_filled <- as.integer(rowSums(tmp[, 1:ncol(tmp)] != 0)),

# count of NA in rows
feat_NA <- apply(X = tmp, MARGIN = 1, FUN = function(x) sum(is.na(x))),  

# optional features -- statistical feature addition

feat_mean <- as.integer(apply(tmp[, 1:ncol(tmp)], 1, mean)),  

feat_median <- as.integer(apply(tmp[, 1:ncol(tmp)], 1, median))

feat_min <- as.integer(apply(tmp[, 1:ncol(tmp)], 1, min))

feat_max <- as.integer(apply(tmp[, 1:ncol(tmp)], 1, max))

feat_LENunique <- as.integer(apply(tmp[, 1:ncol(tmp)], 1, function(x) length(unique(x)))



##------------------------------------------------------------------------------------------------------------------------------------

## add interaction features

# Take interaction features from feature importance from various different algos (Eg : 20 from each)

# Interaction features


int_col <- c()

tmp_int <- tmp_new[ , int_col]



# create + interaction features


for (i in 1:ncol(tmp_int)) {
  
  for (j in (i + 1) : (ncol(tmp_int) + 1)) {
    
    var.x <- colnames(tmp_int)[i]
    
    var.y <- colnames(tmp_int)[j]
    
    var.new <- paste0(var.x, '_plus_', var.y)
    
    tmp_int[ , paste0(var.new)] <- tmp_int[, i] + tmp_int[, j]
    
  }
}




# create - interaction features


for (i in 1:ncol(tmp_int)) {
  
  for (j in (i + 1) : (ncol(tmp_int) + 1)) {
    
    var.x <- colnames(tmp_int)[i]
    
    var.y <- colnames(tmp_int)[j]
    
    var.new <- paste0(var.x, '_minus_', var.y)
    
    tmp_int[ , paste0(var.new)] <- tmp_int[, i] - tmp_int[, j]
    
  }
}





# create * interaction features


for (i in 1:ncol(tmp_int)) {
  
  for (j in (i + 1) : (ncol(tmp_int) + 1)) {
    
    var.x <- colnames(tmp_int)[i]
    
    var.y <- colnames(tmp_int)[j]
    
    var.new <- paste0(var.x, '_mult_', var.y)
    
    tmp_int[ , paste0(var.new)] <- tmp_int[, i] * tmp_int[, j]
    
  }
}





# create / interaction features


for (i in 1:ncol(tmp_int)) {
  
  for (j in (i + 1) : (ncol(tmp_int) + 1)) {
    
    var.x <- colnames(tmp_int)[i]
    
    var.y <- colnames(tmp_int)[j]
    
    var.new <- paste0(var.x, '_divide_', var.y)
    
    tmp_int[, paste0(var.new)] <- tmp_int[, i] / tmp_int[, j]
    
  }
}



tmp_int <- tmp_int[, -int_col]
