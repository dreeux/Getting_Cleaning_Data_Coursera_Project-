require(data.table); require(lubridate); require(caret); require(sqldf); require(xgboost); require(sqldf); require(xlsx); require(Matrix)

train_raw <- fread(input = "D:\\kaggle\\HOMESITE\\Data\\train.csv", data.table = F)

response <- train_raw$QuoteConversion_Flag

response <- as.matrix(response)

train_raw$QuoteConversion_Flag <- NULL

train_raw$QuoteNumber <- NULL


test_raw <- fread(input = "D:\\kaggle\\HOMESITE\\Data\\test.csv", data.table = F)

id <- test_raw$QuoteNumber

test_raw$QuoteNumber <- NULL


tmp <- rbind(train_raw, test_raw)

tmp$Original_Quote_Date <- as.Date(tmp$Original_Quote_Date)

tmp$month <- as.integer(format(tmp$Original_Quote_Date, "%m"))

tmp$year <- as.integer(format(tmp$Original_Quote_Date, "%y"))

tmp$day <- weekdays(as.Date(tmp$Original_Quote_Date))

continous_field <- tmp$SalesField8

tmp$SalesField8 <- NULL

tmp$Original_Quote_Date <- NULL

tmp <- as.matrix(tmp)

# group data => create combinations of a given order
groupData <- function(xmat, degree)
{
  require(foreach, quietly = T)
  # indices of combinations
  xind <- combn(1:ncol(xmat), degree)
  
  # storage structure for the result
  agx <- foreach(ii = 1:ncol(xind), .combine = cbind ) %do%
{
  x <- xmat[,xind[1,ii]]
  for (jj in 2:nrow(xind))
  {
    x <- paste(x, xmat[,xind[jj,ii]], sep = "_")
  }
  x
}
  colnames(agx) <- paste(paste("f", degree, sep = ""), 1:ncol(agx), sep = "_")
  return(agx)
}


# running into memory issues

# double <- groupData(tmp, 2)

tmp_mat <- cbind(tmp)#, double)

tmp_mat_train <- tmp_mat[1:260753 , ]

tmp_mat_test = tmp_mat[(260753+1):434589, ]

response_test = numeric()

# need tmp_mat_train, tmp_mat_test, response, response_test : remove everything

#function for forward stepwise logistic regression fitting to determine optimal features to encode


set.seed(508)

require(glmnet, quietly = T)

require(Matrix, quietly = T)

#initialize optimization metrics-----------------------------------------------------------------------------------------------

cv_max_auc = 0.5 #target minimum.... represents random guess

cv_fold_auc = numeric()

cv_train_auc = numeric()

#initialize feature holding-----------------------------------------------------------------------------------------------------

best_col = numeric()

num_features = numeric()

# big inefficient for loop that does everything -------------------------------------------------------------------------------

for(i in 1:ncol(tmp_mat_train)) 

{

# add columns selected + iteration column

  colName = colnames(tmp_mat_train)[c(best_col, i)] 
  
  vars = as.data.frame(tmp_mat_train[,c(best_col, i)])
  
  colnames(vars) = colName
  
  #encode into sparse model               
  
  vars = sparse.model.matrix(~ . - 1, data = vars)                     
  
  #10 fold logistic regression w/ lasso reg ran to obtain max mean AUC on validation set
  
  for(j in 1:10) 
  
  {
  
    cv_train = cv.glmnet(x = vars, y = response[,1], family = "binomial", type.measure = "auc")
  
    cv_fold_auc[j] = max(cv_train$cvm)
  
  }
  
  cv_train_auc[i] = mean(cv_fold_auc)
  
  #reset cv fold auc
  
  cv_fold_auc = numeric()
  
  #determining if new column is useful.  if so, adding to the model and raising auc bar
  
  if(cv_train_auc[i] > cv_max_auc)
  
  {
    
    #for next iteration: know indecies of the columns to keep
  
    best_col = c(best_col, i)
    
    #store how many important features from the current set to plot against auc
  
    best_features = which(coef(cv_train, s = cv_train$lambda.min) > 0)
  
    num_features[i] = length(best_features)
    
    #raise auc bar
  
    cv_max_auc = cv_train_auc[i]
    
  }
  
  #live update
  
  for(k in 1)
  
  {
  
  print(cat('Feature Loop', i, 'complete.  Max validation AUC:', cv_max_auc, 'Number of features:', num_features[i]))
  
  }
  
}
  
print(best_col) 

tmp_mat_train = tmp_mat_train[,best_col]

# add model training step


##############################################################################################################################

# new approach---------------------------------------------------------------------------------------------------------------

# two way and three way counts

# sparse matrix

# run lasso, same step as listed above

# run xgboost on the selected features

#############################################################################################################################

# new appraoch ------------------------------------------------------------------------------------------------------------

# factor to integer

### Creates a set of new columns for your data set that summarize factor levels by descriptive statistics such as mean,

### sd, skewness etc.

### Variables : train = training set

###             test = test set

###             response = response variable (Hazard in this competition)

###             variables = vector of column names you wise to summarize (T1_V4 for example). Must be strings.

###             metrics = vector of desctriptive statistics you wish to compute for each factor level. Must Be Strings.

### ex: factorToNumeric(train, test, "Hazard", "T1_V4", "mean") will return a column of hazard means by factor level

###     in column T1_V4

### You can specify the test parameter as train to get a column to add to your training set

###### REQUIRES package qdapTools  #####

library(qdapTools)



factorToNumeric <- function(train, test, response, variables, metrics){

  temp <- data.frame(c(rep(0,nrow(test))), row.names = NULL)

  for (variable in variables){

    for (metric in metrics) {

      x <- tapply(train[, response], train[,variable], metric)

      x <- data.frame(row.names(x),x, row.names = NULL)

      temp <- data.frame(temp,round(lookup(test[,variable], x),2))

      colnames(temp)[ncol(temp)] <- paste(metric,variable, sep = "_")

    }

  }

  return (temp[,-1])

}



#### Sample Usage #### 

train <- read.csv("../input/train.csv")

test <- read.csv("../input/test.csv")

#### Returns mean, median, and sd for factor T1_V4 by factor level to be added to the training set.

data.frame(head(train$T1_V4))

head(factorToNumeric(train, train, "Hazard", "T1_V4", c("mean","median","sd")))

#### Returns mean, median, and sd for factor T1_V4 by factor level to be added to the test set.

data.frame(head(test$T1_V4))

head(factorToNumeric(train, test, "Hazard", "T1_V4", c("mean","median","sd")))

##############################################################################################################################

# new approach--------------------------------------------------------------------------------------------------------------



