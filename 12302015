
df_1 <- data.frame(v1 = unique(df$v1), v2 = rep(0, length(unique(df$v1)) ))

for(i in unique(df$v1)){
  
  test <- df[df$v1 == i, ]
  
  tmp <- (test$v2)
  
  df_1[i, 2] <- (unique(tmp))
  
  }



library(caret)


test$Response = 0


## store Id column and remove it from the train and test data


testId = test$Id

train$Id = test$Id = NULL


tmp = rbind(train, test)


cat.var.names <- c(paste("Product_Info_", c(1:3,5:7), sep=""), paste("Employment_Info_", c(2,3,5), sep=""),
                   
		   paste("InsuredInfo_", 1:7, sep=""), paste("Insurance_History_", c(1:4,7:9), sep=""), 
                   
                   "Family_Hist_1", paste("Medical_History_", c(2:14, 16:23, 25:31, 33:41), sep=""))


tmp_factors <- tmp[, cat.var.names]
  

len = length(names(tmp_factors))


for (i in 1:len) {
    
  levels <- unique(tmp_factors[[i]])
  
  tmp_factors[, i] <- as.factor(tmp_factors[, i], levels = levels)

    
  }


tmp <- tmp[, !( names(tmp) %in% names(tmp_factors))]


tmp_new <- cbind(tmp, tmp_factors)


# check row numbers


train <- tmp_new[c(1:59382),]

test <- tmp_new[c(59383:79148),]



## create mlr task and convert factors to dummy features


trainTask = makeRegrTask(data = train, target = "Response")

trainTask = createDummyFeatures(trainTask)

testTask = makeRegrTask(data = test, target = "Response")

testTask = createDummyFeatures(testTask)


## create mlr learner


set.seed(12302015)

lrn = makeLearner("regr.xgboost")

lrn$par.vals = list(

  #nthread             = 30,

  nrounds             = 4000,

  print.every.n       = 20,

  objective           = "reg:linear",

  depth = 21,

  colsample_bytree = 0.66,

  min_child_weight = 3,

  subsample = 0.71

)


# missing values will be imputed by their median


lrn = makeImputeWrapper(lrn, classes = list(numeric = imputeMedian(), integer = imputeMedian()))

## Create Evaluation Function


SQWKfun = function(x = seq(1.5, 7.5, by = 1), pred) {

  preds = pred$data$response

  true = pred$data$truth 

  cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))

  preds = as.numeric(Hmisc::cut2(preds, cuts))

  err = Metrics::ScoreQuadraticWeightedKappa(preds, true, 1, 8)

  return(-err)

}


SQWK = makeMeasure(id = "SQWK", minimize = FALSE, properties = c("regr"), best = 1, worst = 0,

  fun = function(task, model, pred, feats, extra.args) {

    return(-SQWKfun(x = seq(1.5, 7.5, by = 1), pred))

  })



## This is how you could do hyperparameter tuning

# # 1) Define the set of parameters you want to tune (here 'eta')



 ps = makeParamSet(

   makeNumericParam("eta", lower = 0.01, upper = 0.03)

 )


# # 2) Use 3-fold Cross-Validation to measure improvements


 rdesc = makeResampleDesc("CV", iters = 3L)


# # 3) Here we use Random Search (with 10 Iterations) to find the optimal hyperparameter


 ctrl =  makeTuneControlRandom(budget = 10, maxit = 10)


# # 4) now use the learner on the training Task with the 3-fold CV to optimize your set of parameters and evaluate it with SQWK


 res = tuneParams(lrn, task = trainTask, resampling = rdesc, par.set = ps, control = ctrl, measures = SQWK)


 res


# # 5) set the optimal hyperparameter


 lrn = setHyperPars(lrn, par.vals = res$x)


## now try to find the optimal cutpoints that maximises the SQWK measure based on the Cross-Validated predictions


cv = crossval(lrn, trainTask, iter = 4, measures = SQWK, show.info = TRUE)


optCuts = optim(seq(1.5, 7.5, by = 1), SQWKfun, pred = cv$pred)


optCuts



## now train the model on all training data


tr = train(lrn, trainTask)



## predict using the optimal cut-points 


pred = predict(tr, testTask)


preds = as.numeric(Hmisc::cut2(pred$data$response, c(-Inf, optCuts$par, Inf)))


table(preds)


## create submission file


submission = data.frame(Id = testId)


submission$Response = as.integer(preds)


write.csv(submission,   , row.names = FALSE)


######################################################################################################################################

for non linear algos replace a cat. var by number of times

it appears in the set

GBM with out-of-fold treatment of high-cardinality feature 

performs very well


##############################################################################################################################
##############################################################################################################################


# read data-----------------------------------------------------------------------------------------------

require(data.table); require(xgboost); require(caret); require(doParallel); require(readr)

train_raw <- fread("D:\\kaggle\\BNP\\DATA\\train.csv", data.table = F)

test_raw <- fread("D:\\kaggle\\BNP\\DATA\\test.csv", data.table = F)


response <- train_raw$target

id <- test_raw$ID

train_raw$target <- NULL

train_raw$ID <- NULL

test_raw$ID <- NULL

tmp <- rbind(train_raw, test_raw)

feature.names <- names(tmp)

#######################################################################################################

for (f in feature.names) {
  
  if (class(tmp[[f]]) == "character") {
    
    levels <- unique(c(tmp[[f]]))
    
    tmp[[f]] <- as.integer(factor(tmp[[f]], levels=levels))
    
    
  }
}


# count of NA------------------------------------------------------------------------------------------

row_NA <- apply(tmp, 1, function(x) sum(is.na(x)))

tmp$row_NA <- row_NA

tmp[is.na(tmp)] <- -1


  # create train and test sets---------------------------------------------------------------------------- 

# from train create training and hold out---------------------------------------------------------------

# create 20 % validation set-----------------------------------------------------------------------------

# from now on use data from training instead of train--------------------------------------------------


train <- tmp[c(1:nrow(train_raw)), ]

test <- tmp[c((nrow(train_raw) +1) : nrow(tmp)), ]


train$target <- response

split <- createDataPartition(y = train$target, p = 0.8, list = F)


training <- train[split, ]; training$target <- NULL

response_tr <- response[split]

holdout <- train[-split, ]; holdout$target <- NULL

response_hol <- response[-split]

h <- sample(nrow(training), 1000)

dval<-xgb.DMatrix(data=data.matrix(training[h,]),label=response_tr[h])

dtrain<-xgb.DMatrix(data=data.matrix(training[-h,]),label=response_tr[-h])

watchlist<-list(val=dval,train=dtrain)


param <- list(
  
  # general , non specific params - just guessing
  
  "objective"  = "binary:logistic"
  
  , "eval_metric" = "logloss"
  
  , "eta" = 0.01
  
  , "subsample" = 0.8
  
  , "colsample_bytree" = 0.8
  
  , "min_child_weight" = 1
  
  , "max_depth" = 10
)

start <- Sys.time()

cl <- makeCluster(4); registerDoParallel(cl)

set.seed(1*14*16)

# train----------------------------------------------------------------------------------------------

clf <- xgb.train(   params              = param,
                    
                    data                = dtrain,
                    
                    nrounds             = 4000,
                    
                    verbose             = 1,  #1
                    
                    early.stop.round    = 1000,
                    
                    watchlist           = watchlist,
                    
                    maximize            = F,
                    
                    nthread = 4
                    
)


# local validation------------------------------------------------------------------------------------

pred <- predict(clf, data.matrix(holdout[,feature.names]))

# LogLoss Function

LogLoss <- function(actual, predicted, eps=0.00001) {
  
  predicted <- pmin(pmax(predicted, eps), 1-eps)
  
  -1/length(actual)*(sum(actual*log(predicted)+(1-actual)*log(1-predicted)))
  
}

LogLoss(response_hol, pred)

# submission ------------------------------------------------------------------------------------------

pred <- predict(clf, data.matrix(test[,feature.names]))

submission <- data.frame(ID = id, PredictedProb = pred)

write_csv(submission, "D:\\kaggle\\BNP\\submission\\add_na.csv")
